@article{Figueredo:2009dg,
   author = {Figueredo, A.~J. and Wolf, P. S.~A.},
   title = {Assortative pairing and life history strategy - a cross-cultural study.},
   journal = {Human Nature},
   volume = {20},
   pages = {317-330},
   year = {2009},
  note={\href{https://doi.org/10.1007/s12110-009-9068-2}{[CrossRef]\!}}
}
  note={\href{https://doi.org/10.1007/s12110-009-9068-2}{[CrossRef]\!}}
@article{r1,
  title={Intriguing properties of neural networks.},
  author={Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  volume={arXiv: 1312.6199},
  journal={arXiv},
  note={Available online: \url{http://arxiv.org/abs/1312.6199} (accessed on 19 Feb 2014).},
  year={2014}
}
@article{r2,
  title={Explaining and harnessing adversarial examples.},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  volume={arXiv: 1412.6572},
  journal={arXiv},
  note={Available online: \url{http://arxiv.org/abs/1412.6572} (accessed on 20 Mar 2015).},
  year={2015}
}
@article{r3,
  title={Adversarial machine learning at scale.},
  author={Kurakin, Alexey and Goodfellow, Ian and Bengio, Samy},
  volume={arXiv: 1611.01236},
  journal={arXiv},
  note={Available online: \url{http://arxiv.org/abs/1611.01236} (accessed on 11 Feb 2017).},
  year={2017}
}
@inproceedings{r4,
  title={Deepfool: a simple and accurate method to fool deep neural networks},
  author={Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Frossard, Pascal},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2574--2582},
  year={2016},
  note={\href{https://openaccess.thecvf.com/content_cvpr_2016/html/Moosavi-Dezfooli_DeepFool_A_Simple_CVPR_2016_paper.html}{[CrossRef]\!}}
}


@inproceedings{r5,
  title={Universal adversarial perturbations},
  author={Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Fawzi, Omar and Frossard, Pascal},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1765--1773},
  year={2017},
note={\href{https://openaccess.thecvf.com/content_cvpr_2017/html/Moosavi-Dezfooli_Universal_Adversarial_Perturbations_CVPR_2017_paper.html}{[CrossRef]\!}}
}
@article{r6,
  title={Tailoring Adversarial Attacks on Deep Neural Networks for Targeted Class Manipulation Using DeepFool Algorithm.},
  author={Labib, SM and Mondal, Joyanta Jyoti and Manab, Meem Arafat},
  volume={arXiv: 2310.13019},
  journal={arXiv},
  note={Available online: \url{http://arxiv.org/abs/2310.13019} (accessed on 30 Aug 2024).},
  year={2024}
}
@inproceedings{r7,
  title={Generating Targeted Adversarial Attacks and Assessing their Effectiveness in Fooling Deep Neural Networks},
  author={Gajjar, Shivangi and Hati, Avik and Bhilare, Shruti and Mandal, Srimanta},
  booktitle={2022 IEEE International Conference on Signal Processing and Communications (SPCOM)},
  pages={1--5},
  year={2022},
  organization={IEEE},
note={\href{https://doi.org/10.1109/SPCOM55316.2022.9840784}{[CrossRef]\!}}
}






@article{ILSVRC2012,
  title={Imagenet large scale visual recognition challenge},
  author={Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
  journal={International journal of computer vision},
  volume={115},
  pages={211--252},
  year={2015},
  publisher={Springer},
note={\href{https://doi.org/10.7717/peerj-cs.1868}{[CrossRef]\!}}
}
@article{vyas2024designing,
  title={Designing defensive techniques to handle adversarial attack on deep learning based model},
  author={Vyas, Dhairya and Kapadia, Viral V},
  journal={PeerJ Computer Science},
  volume={10},
  pages={e1868},
  year={2024},
  publisher={PeerJ Inc.},
note={\href{https://doi.org/10.7717/peerj-cs.1868}{[CrossRef]\!}}
}
@inproceedings{ali2024balancing,
  title={Balancing Efficiency and Effectiveness: Adversarial Example Generation in Pneumonia Detection},
  author={Ali, Mohd Adli Md and Isa, Hafizah Noor and others},
  booktitle={2024 IEEE Symposium on Wireless Technology \& Applications (ISWTA)},
  pages={206--210},
  year={2024},
  organization={IEEE},
note={\href{https://doi.org/10.1109/ISWTA62130.2024.10651652}{[CrossRef]\!}}
}
@article{gao2021scaling,
  title={Scaling deep contrastive learning batch size under memory limited setup.},
  author={Gao, Luyu and Zhang, Yunyi and Han, Jiawei and Callan, Jamie},
  volume={arXiv: 2101.06983},
  journal={arXiv},
  note={Available online: \url{http://arxiv.org/abs/2101.06983} (accessed on 14 Jun 2021).},
  year={2021}
}
@article{singh2024revisiting,
  title={Revisiting adversarial training for imagenet: Architectures, training and generalization across threat models},
  author={Singh, Naman Deep and Croce, Francesco and Hein, Matthias},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024},
note={\href{https://proceedings.neurips.cc/paper_files/paper/2023/hash/2d3b007613940def7a5ec9d6d635937b-Abstract-Conference.html}{[CrossRef]\!}}
}
@inproceedings{guesmi2024advart,
  title={Advart: Adversarial art for camouflaged object detection attacks},
  author={Guesmi, Amira and Bilasco, Ioan Marius and Shafique, Muhammad and Alouani, Ihsen},
  booktitle={2024 IEEE International Conference on Image Processing (ICIP)},
  pages={666--672},
  year={2024},
  organization={IEEE},
note={\href{https://doi.org/10.1109/ICIP51287.2024.10648014}{[CrossRef]\!}}
}


@article{mkadry2017towards,
  title={Towards deep learning models resistant to adversarial attacks},
  author={M{\k{a}}dry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  journal={stat},
  volume={1050},
  number={9},
  year={2017}
}
@incollection{kurakin2018adversarial,
  title={Adversarial examples in the physical world},
  author={Kurakin, Alexey and Goodfellow, Ian J and Bengio, Samy},
  booktitle={Artificial intelligence safety and security},
  pages={99--112},
  year={2018},
  publisher={Chapman and Hall/CRC},
note={\href{https://www.taylorfrancis.com/chapters/edit/10.1201/9781351251389-8/adversarial-examples-physical-world-alexey-kurakin-ian-goodfellow-samy-bengio}{[CrossRef]\!}}
}

@article{wong2020fast,
  title={Fast is better than free: Revisiting adversarial training.},
  author={Wong, Eric and Rice, Leslie and Kolter, J Zico},
  volume={arXiv: 2001.03994},
  journal={arXiv},
  note={Available online: \url{http://arxiv.org/abs/2001.03994} (accessed on 12 Jan 2020).},
  year={2020}
}
@inproceedings{croce2020reliable,
  title={Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks},
  author={Croce, Francesco and Hein, Matthias},
  booktitle={International conference on machine learning},
  pages={2206--2216},
  year={2020},
  organization={PMLR},
note={\href{https://proceedings.mlr.press/v119/croce20b.html}{[CrossRef]\!}}
}

@inproceedings{modas2019sparsefool,
  title={Sparsefool: a few pixels make a big difference},
  author={Modas, Apostolos and Moosavi-Dezfooli, Seyed-Mohsen and Frossard, Pascal},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={9087--9096},
  year={2019},
note={\href{https://openaccess.thecvf.com/content_CVPR_2019/html/Modas_SparseFool_A_Few_Pixels_Make_a_Big_Difference_CVPR_2019_paper.html}{[CrossRef]\!}}
}

@article{su2019one,
  title={One pixel attack for fooling deep neural networks},
  author={Su, Jiawei and Vargas, Danilo Vasconcellos and Sakurai, Kouichi},
  journal={IEEE Transactions on Evolutionary Computation},
  volume={23},
  number={5},
  pages={828--841},
  year={2019},
  publisher={IEEE},
note={\href{https://doi.org/10.1109/TEVC.2019.2890858}{[CrossRef]\!}}
}

@inproceedings{pomponi2022pixle,
  title={Pixle: a fast and effective black-box attack based on rearranging pixels},
  author={Pomponi, Jary and Scardapane, Simone and Uncini, Aurelio},
  booktitle={2022 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--7},
  year={2022},
  organization={IEEE},
  note={\href{https://doi.org/10.1109/IJCNN55064.2022.9892966}{[CrossRef]\!}}

}


@article{gao2020patch,
  title={Patch-wise++ perturbation for adversarial targeted attacks.},
  author={Gao, Lianli and Zhang, Qilong and Song, Jingkuan and Shen, Heng Tao},
  year={2020},
  journal={arXiv},
  volume={arXiv: 2012.15503},
  note={Available online: \url{http://arxiv.org/abs/2012.15503} (accessed on 8 Jun 2021).}

}





@article{costa2024deep,
  title={How deep learning sees the world: A survey on adversarial attacks \& defenses},
  author={Costa, Joana C and Roxo, Tiago and Proen{\c{c}}a, Hugo and In{\'a}cio, Pedro RM},
  journal={IEEE Access},
  year={2024},
  publisher={IEEE},
    note={\href{https://doi.org/10.1109/ACCESS.2024.3395118}{[CrossRef]\!}}
}


@article{deng2024understanding,
  title={Understanding and improving ensemble adversarial defense},
  author={Deng, Yian and Mu, Tingting},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024},
note={\href{https://proceedings.neurips.cc/paper_files/paper/2023/file/b589d92785e39486e978fa273d0dc343-Paper-Conference.pdf}{[CrossRef]\!}}
}

@article{shi2024attack,
  title={Attack-invariant attention feature for adversarial defense in hyperspectral image classification},
  author={Shi, Cheng and Liu, Ying and Zhao, Minghua and Pun, Chi-Man and Miao, Qiguang},
  journal={Pattern Recognition},
  volume={145},
  pages={109955},
  year={2024},
  publisher={Elsevier},
    note={\href{https://doi.org/10.1016/j.patcog.2023.109955}{[CrossRef]\!}}
}

@article{kuang2024defense,
  title={Defense against adversarial attacks using topology aligning adversarial training},
  author={Kuang, Huafeng and Liu, Hong and Lin, Xianming and Ji, Rongrong},
  journal={IEEE Transactions on Information Forensics and Security},
  year={2024},
  publisher={IEEE},
    note={\href{https://doi.org/10.1109/TIFS.2024.3359820}{[CrossRef]\!}}
}

@article{jung2024adversarial,
  title={Adversarial example denoising and detection based on the consistency between Fourier-transformed layers},
  author={Jung, Seunghwan and Chung, Minyoung and Shin, Yeong-Gil and others},
  journal={Neurocomputing},
  volume={606},
  pages={128351},
  year={2024},
  publisher={Elsevier},
 note={\href{https://doi.org/10.1016/j.neucom.2024.128351}{[CrossRef]\!}}
}
@article{shi2024adversarial,
  title={Adversarial self-training improves robustness and generalization for gradual domain adaptation},
  author={Shi, Lianghe and Liu, Weiwei},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024},
 note={\href{https://proceedings.neurips.cc/paper_files/paper/2023/file/75b0edb869e2cd509d64d0e8ff446bc1-Paper-Conference.pdf}{[CrossRef]\!}}
}



@article{qin2019adversarial,
  title={Adversarial robustness through local linearization},
  author={Qin, Chongli and Martens, James and Gowal, Sven and Krishnan, Dilip and Dvijotham, Krishnamurthy and Fawzi, Alhussein and De, Soham and Stanforth, Robert and Kohli, Pushmeet},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019},
 note={\href{https://proceedings.neurips.cc/paper_files/paper/2019/file/0defd533d51ed0a10c5c9dbf93ee78a5-Paper.pdf}{[CrossRef]\!}}
}
@article{najafi2019robustness,
  title={Robustness to adversarial perturbations in learning from incomplete data},
  author={Najafi, Amir and Maeda, Shin-ichi and Koyama, Masanori and Miyato, Takeru},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019},
 note={\href{https://proceedings.neurips.cc/paper_files/paper/2019/file/60ad83801910ec976590f69f638e0d6d-Paper.pdf}{[CrossRef]\!}}
}

@article{naseem2024trans,
  title={Trans-IFFT-FGSM: a novel fast gradient sign method for adversarial attacks},
  author={Naseem, Muhammad Luqman},
  journal={Multimedia Tools and Applications},
  pages={1--21},
  year={2024},
  publisher={Springer},
 note={\href{https://doi.org/10.1007/s11042-024-18475-7}{[CrossRef]\!}}
}

@article{chakraborty2021survey,
  title={A survey on adversarial attacks and defences},
  author={Chakraborty, Anirban and Alam, Manaar and Dey, Vishal and Chattopadhyay, Anupam and Mukhopadhyay, Debdeep},
  journal={CAAI Transactions on Intelligence Technology},
  volume={6},
  number={1},
  pages={25--45},
  year={2021},
  publisher={Wiley Online Library},
 note={\href{https://doi.org/10.1049/cit2.12028}{[CrossRef]\!}}
}

@article{ling2023improving,
  title={Improving the transferability of adversarial samples with channel switching},
  author={Ling, Jie and Chen, Xiaohuan and Luo, Yu},
  journal={Applied Intelligence},
  volume={53},
  number={24},
  pages={30580--30592},
  year={2023},
  publisher={Springer},
 note={\href{https://doi.org/10.1007/s10489-023-05160-9}{[CrossRef]\!}}
}

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee},
 note={\href{https://doi.org/10.1109/5.726791}{[CrossRef]\!}}
}

@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Toronto, ON, Canada},

}


@article{elsheikh2024accuracy,
	title={Accuracy is not enough: a heterogeneous ensemble model versus FGSM attack},
	author={Elsheikh, Reham A and Mohamed, MA and Abou-Taleb, Ahmed Mohamed and Ata, Mohamed Maher},
	journal={Complex \& Intelligent Systems},
	volume={10},
	number={6},
	pages={8355--8382},
	year={2024},
	publisher={Springer}
}


@article{feng2025sami,
	title={SAMI-FGSM: Towards Transferable Attacks with Stochastic Gradient Accumulation.},
	author={Feng, Haolang and Chen, Yuling and Huang, Yang and Wang, Xuewei and Sang, Haiwei},
	journal={Computers, Materials \& Continua},
	volume={84},
	number={3},
	year={2025}
}
@INPROCEEDINGS{11150836,
	author={Antonio, Elbren O.},
	booktitle={2025 IEEE 8th International Conference on Electrical, Control and Computer Engineering (InECCE)}, 
	title={FGSM Attack Impact on MNIST Classifiers via PyTorch Lightning}, 
	year={2025},
	volume={},
	number={},
	pages={617-621},
	keywords={Deep learning;Training;Threat modeling;Sensitivity;Perturbation methods;Lightning;Robustness;Reproducibility of results;Convolutional neural networks;Standards;Adversarial Attacks;FGSM;CNN;MNIST;PyTorch Lightning;Model Robustness;Deep Learning;Adversarial Examples},
	doi={10.1109/InECCE64959.2025.11150836}}


@InProceedings{10.1007/978-981-97-9743-1_17,
	author="Hemashree, P.
	and Padmavathi, G.",
	editor="Shankar Sriram, V.S.
	and H., Anila Glory
	and Li, Gang
	and Pokhrel, Shiva Raj",
	title="Enhancing FGSM Attacks with Genetic Algorithms for Robust Adversarial Examples in Remote Sensing Image Classification Systems",
	booktitle="Applications and Techniques in Information Security",
	year="2025",
	publisher="Springer Nature Singapore",
	address="Singapore",
	pages="229--243",
	abstract="Adversarial attacks present significant threats to the robustness and security of neural networks, particularly in Remote Sensing Image (RSI) classification. The Fast Gradient Sign Method (FGSM) is a conventional technique for creating adversarial examples by perturbing input data along the gradient of the loss function. However, FGSM's limitations, including its linearity assumption and fixed perturbation magnitude, often resulting in suboptimal adversarial examples. This paper presents an innovative approach to enhance FGSM by integrating it with Genetic Algorithm (GA) which optimizes the epsilon value iteratively to improve the attack's success rate and robustness. Experiments on EuroSAT and UCMerced-LandUse datasets utilizing deep learning architectures like ResNet, EfficientNet, and MobileNet demonstrate that the GA-enhanced FGSM significantly outperforms the baseline FGSM across several performance metrics, such as accuracy, precision, recall, F1 score, and loss. For instance, the MobileNet model's accuracy dropped from 95.23{\%} to 81.67{\%} and 75{\%} under FGSM and FGSM{\thinspace}+{\thinspace}GA attacks, respectively. Similarly, the performance of other models also declined significantly when subjected to the FGSM+GA attack. This integration of FGSM with GA not only boosts the efficacy of adversarial attacks but also offers insights into the underlying mechanisms of neural network vulnerabilities in remote sensing applications.",
	isbn="978-981-97-9743-1"
}


@article{ma2025mitigating,
	title={Mitigating FGSM-based white-box attacks using convolutional autoencoders for face recognition},
	author={Ma, Jiahuai and Wilson, Alan},
	year={2025}
}


@InProceedings{10.1007/978-3-032-00644-8_14,
	author="Catillo, Marta
	and Pecchia, Antonio
	and Villano, Umberto",
	editor="Skopik, Florian
	and Naessens, Vincent
	and De Sutter, Bjorn",
	title="Detection of Adversarial Examples by Adversarial Training: A Study on the Suitability of FGSM for Hardening NIDS Against Problem-Space Attacks",
	booktitle="Availability, Reliability and Security",
	year="2025",
	publisher="Springer Nature Switzerland",
	address="Cham",
	pages="232--249",
	abstract="Network intrusion detection systems (NIDS) can leverage machine and deep learning techniques to monitor network traffic and recognize potential intrusions. Although valuable, deep learning-based NIDS are vulnerable to adversarial attacks. Adversarial training, which consists in integrating adversarial examples into the training process, is a means to improve the robustness of NIDS. The literature has largely demonstrated that adversarial examples can be successfully crafted in the feature space through the perturbation of the network traffic features used by NIDS. This paper puts forward the intuition that the use of feature-space perturbations for improving the robustness of NIDS by adversarial training is questionable. This aspect is exacerbated when the network traffic is perturbed prior to the feature extraction step, in the problem space. The experiment reported in the paper is based on the application of both a feature-space and a problem-space adversarial attack to normal and Denial of Service network traffic collected in a controlled testbed. The results obtained aim to promote a critical reflection on the use of feature-space perturbations in the context of network intrusions and suggest the need for more foundational research on protecting NIDS from problem-space adversarial attacks.",
	isbn="978-3-032-00644-8"
}


@Article{su15021233,
	AUTHOR = {You, Haotian and Lu, Yufang and Tang, Haihua},
	TITLE = {Plant Disease Classification and Adversarial Attack Using SimAM-EfficientNet and GP-MI-FGSM},
	JOURNAL = {Sustainability},
	VOLUME = {15},
	YEAR = {2023},
	NUMBER = {2},
	ARTICLE-NUMBER = {1233},
	URL = {https://www.mdpi.com/2071-1050/15/2/1233},
	ISSN = {2071-1050},
	ABSTRACT = {Plant diseases have received common attention, and deep learning has also been applied to plant diseases. Deep neural networks (DNNs) have achieved outstanding results in plant diseases. Furthermore, DNNs are very fragile, and adversarial attacks in image classification deserve much attention. It is important to detect the robustness of DNNs through adversarial attacks. The paper firstly improves the EfficientNet by adding the SimAM attention module. The SimAM-EfficientNet is proposed in this paper. The experimental results show that the accuracy of the improved model on PlantVillage reaches 99.31%. The accuracy of ResNet50 is 98.33%. The accuracy of ResNet18 is 98.31%. The accuracy of DenseNet is 98.90%. In addition, the GP-MI-FGSM adversarial attack algorithm improved by gamma correction and image pyramid in this paper can increase the success rate of attack. The model proposed in this paper has an error rate of 87.6% whenattacked by the GP-MI-FGSM adversarial attack algorithm. The success rate of GP-MI-FGSM proposed in this paper is higher than other adversarial attack algorithms, including FGSM, I-FGSM, and MI-FGSM.},
	DOI = {10.3390/su15021233}
}


@INPROCEEDINGS{10688539,
	author={Wang, Kebei and Govindarasu, Manimaran},
	booktitle={2024 IEEE Power & Energy Society General Meeting (PESGM)}, 
	title={FGSM-based Synthetic Data Generation Technique and Application to Anomaly Detection in Smart Grid}, 
	year={2024},
	volume={},
	number={},
	pages={1-5},
	keywords={Machine learning algorithms;Accuracy;Protocols;Smart grids;Distributed power generation;Security;Anomaly detection;Optimization;Synthetic data;Testing;DNP3;cybersecurity;ADS;ML-ADS;GAN;FGSM;synthetic data},
	doi={10.1109/PESGM51994.2024.10688539}}



@InProceedings{10.1007/978-981-97-3523-5_26,
	author="Sen, Jaydip",
	editor="Nanda, Umakanta
	and Tripathy, Asis Kumar
	and Sahoo, Jyoti Prakash
	and Sarkar, Mahasweta
	and Li, Kuan-Ching",
	title="The FGSM Attack on Image Classification Models and Distillation as Its Defense",
	booktitle="Advances in Distributed Computing and Machine Learning",
	year="2024",
	publisher="Springer Nature Singapore",
	address="Singapore",
	pages="347--360",
	isbn="978-981-97-3523-5"
}


@article{sen2023adversarial,
	title={Adversarial attacks on image classification models: FGSM and patch attacks and their impact},
	author={Sen, Jaydip and Dasgupta, Subhasis},
	journal={arXiv preprint arXiv:2307.02055},
	year={2023}
}


@inproceedings{lupart2023study,
	title={A study on FGSM adversarial training for neural retrieval},
	author={Lupart, Simon and Clinchant, St{\'e}phane},
	booktitle={European Conference on Information Retrieval},
	pages={484--492},
	year={2023},
	organization={Springer}
}


@article{wang2022ab,
	title={AB-FGSM: AdaBelief optimizer and FGSM-based approach to generate adversarial examples},
	author={Wang, Yixiang and Liu, Jiqiang and Chang, Xiaolin and Wang, Jianhua and Rodriguez, Ricardo J},
	journal={Journal of Information Security and Applications},
	volume={68},
	pages={103227},
	year={2022},
	publisher={Elsevier}
}

@article{zhang2025adversarial,
	title={Adversarial attacks of vision tasks in the past 10 years: A survey},
	author={Zhang, Chiyu and Zhou, Lu and Xu, Xiaogang and Wu, Jiafei and Liu, Zhe},
	journal={ACM Computing Surveys},
	volume={58},
	number={2},
	pages={1--42},
	year={2025},
	publisher={ACM New York, NY}
}

@article{vadillo2025adversarial,
	title={Adversarial attacks in explainable machine learning: A survey of threats against models and humans},
	author={Vadillo, Jon and Santana, Roberto and Lozano, Jose A},
	journal={Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
	volume={15},
	number={1},
	pages={e1567},
	year={2025},
	publisher={Wiley Online Library}
}

@article{zheng2025blackboxbench,
	title={Blackboxbench: A comprehensive benchmark of black-box adversarial attacks},
	author={Zheng, Meixi and Yan, Xuanchen and Zhu, Zihao and Chen, Hongrui and Wu, Baoyuan},
	journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
	year={2025},
	publisher={IEEE}
}

@inproceedings{ma2025jailbreaking,
	title={Jailbreaking prompt attack: A controllable adversarial attack against diffusion models},
	author={Ma, Jiachen and Li, Yijiang and Xiao, Zhiqing and Cao, Anda and Zhang, Jie and Ye, Chao and Zhao, Junbo},
	booktitle={Findings of the Association for Computational Linguistics: NAACL 2025},
	pages={3141--3157},
	year={2025}
}

@article{melendez2025adversarial,
	title={Adversarial attacks in demand-side electricity markets},
	author={Melendez, Kevin A and Matamala, Yolanda},
	journal={Applied Energy},
	volume={377},
	pages={124615},
	year={2025},
	publisher={Elsevier}
}

@article{yang2025tanh,
	title={tanh As a robust feature scaling method in training deep learning models with imbalanced data},
	author={Yang, Aijia and Zhang, Min and Chen, Huai and Li, Taihao and Liu, Shupeng and Xu, Xiaoyin},
	journal={Pattern Recognition},
	pages={111746},
	year={2025},
	publisher={Elsevier}
}

@inproceedings{niu2025enhancing,
	title={Enhancing Small Object Detection with Dynamic Tanh for Pharmaceutical Appearance Classification},
	author={Niu, Hongkai},
	booktitle={2025 4th International Conference on Intelligent Mechanical and Human-Computer Interaction Technology (IHCIT)},
	pages={189--192},
	year={2025},
	organization={IEEE}
}

@article{pelekis2025adversarial,
	title={Adversarial machine learning: a review of methods, tools, and critical industry sectors.},
	author={Pelekis, Sotiris and Koutroubas, Thanos and Blika, Afroditi and Berdelis, Anastasis and Karakolis, Evangelos and Ntanos, Christos and Spiliotis, Evangelos and Askounis, Dimitris},
	journal={Artificial Intelligence Review},
	number={8},
	year={2025}
}